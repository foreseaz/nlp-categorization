{"selfpaced": true, "topic_name": ["Artificial Intelligence"], "description": "\n\n                    \n                    \n                    \r\n\r\nThis is the first course in the 3-course Machine Learning Series and is offered at Georgia Tech as CS7641.\r\nPlease note that this is first course is different in structure compared to most Udacity CS courses. There is a final project at the end of the course, and there are no programming quizzes throughout this course.\r\nThis course covers Supervised Learning, a machine learning task that makes it possible for your phone to recognize your voice, your email to filter spam, and for computers to learn a bunch of other cool stuff.\r\nSupervised Learning is an important component of all kinds of technologies, from stopping credit card fraud, to finding faces in camera images, to recognizing spoken language. Our goal is to give you the skills that you need to understand these technologies and interpret their output, which is important for solving a range of data science problems. And for surviving a robot uprising.\r\nSeries Information: Machine Learning is a graduate-level series of 3 courses, covering the area of Artificial Intelligence concerned with computer programs that modify and improve their performance through experiences.\r\n\r\nMachine Learning 1: Supervised Learning\u00a0(this course)\r\nMachine Learning 2: Unsupervised Learning\r\nMachine Learning 3: Reinforcement Learning\r\n\r\nIf you are new to Machine Learning, we recommend you take these 3 courses in order.\r\nThe entire series is taught as a lively and rigorous dialogue between two eminent Machine Learning professors and friends: Professor Charles Isbell (Georgia Tech) and Professor Michael Littman (Brown University).\r\n\r\nWhy Take This Course?\r\n\r\nIn this course, you will gain an understanding of a variety of topics and methods in Supervised Learning. Like function approximation in general, Supervised Learning prompts you to make generalizations based on fundamental assumptions about the world.\r\nMichael: So why wouldn't you call it \"function induction?\"Charles: Because someone said \"supervised learning\" first.\r\nTopics covered in this course include: Decision trees, neural networks, instance-based learning, ensemble learning, computational learning theory, Bayesian learning, and many other fascinating machine learning concepts.\r\nIn your final project, you will explore important techniques in Supervised Learning, and apply your knowledge to analyze how algorithms behave under a variety of circumstances.\r\n\r\nPrerequisites and Requirements\r\n\r\nA strong familiarity with Probability Theory, Linear Algebra and Statistics is required. An understanding ofIntro to Statistics, especially\u00a0Lessons 8, 9 and 10, would be helpful.\r\nStudents should also have some experience in programming (perhaps through\u00a0Introduction to CS) and a familiarity with Neural Networks (as covered in\u00a0Introduction to Artificial Learning).\r\n\r\nSee the\u00a0Technology Requirements\u00a0for using Udacity\r\n\n\n                                            Syllabus\n                        Lesson 0: Machine Learning is the ROX\r\n\r\nDefinition of Machine Learning\r\nSupervised learning\r\nInduction and deduction\r\nUnsupervised learning\r\nReinforcement learning\r\n\r\nLesson 1: Decision Trees\r\n\r\nClassification and Regression overview\r\nClassification learning\r\nExample: Dating\r\nRepresentation\r\nDecision trees learning\r\nDecision tree expressiveness\r\nID3 algorithm\r\nID3 bias\r\nDecision trees and continuous attributes\r\n\r\nLesson 2: Regression and Classification\r\n\r\nRegression and function approximation\r\nLinear regression and best fit\r\nOrder of polynomial\r\nPolynomial regression\r\nCross validation\r\n\r\nLesson 3: Neural Networks\r\n\r\nArtificial neural networks\r\nPerceptron units\r\nXOR as perceptron network\r\nPerceptron training\r\nGradient descent\r\nComparison of learning rules\r\nSigmoid function\r\nOptimizing weights\r\nRestriction bias\r\nPreference bias\r\n\r\nLesson 4: Instance-Based Learning\r\n\r\nInstance based learning before\r\nInstance based learning now\r\nK-NN algorithm\r\nWon\u2019t you compute my neighbors?\r\nDomain K-NNowledge\r\nK-NN bias\r\nCurse of dimensionality\r\n\r\nLesson 5: Ensemble B&amp;B\r\n\r\nEnsemble learning: Boosting\r\nEnsemble learning algorithm\r\nEnsemble learning outputs\r\nWeak learning\r\nBoosting in code\r\nWhen D agrees\r\n\r\nLesson 6: Kernel Methods and Support Vector Machines (SVM)s\r\n\r\nSupport Vector Machines\r\nOptimal separator\r\nSVMs: Linearly married\r\nKernel methods\r\n\r\nLesson 7: Computational Learning Theory\r\n\r\nComputational Learning Theory\r\nLearning theory\r\nResources in Machine Learning\r\nDefining inductive learning\r\nTeacher with constrained queries\r\nLearner with constrained queries\r\nLearner with mistake bounds\r\nVersion spaces\r\nPAC learning\r\nEpsilon exhausted\r\nHaussler theorem\r\n\r\nLesson 8: VC Dimensions\r\n\r\nInfinite hypothesis spaces\r\nPower of a hypothesis space\r\nWhat does VC stand for?\r\nInternal training\r\nLinear separators\r\nThe ring\r\nPolygons\r\nSampling complexity\r\nVC of finite H\r\n\r\nLesson 9: Bayesian Learning\r\n\r\nBayes Rule\r\nBayesian learning\r\nBayesian learning in action!\r\nNoisy data\r\nBest hypothesis\r\nMinimum description length\r\nBayesian classification\r\n\r\nLesson 10: Bayesian Inference\r\n\r\nJoint distribution\r\nAdding attributes\r\nConditional independence\r\nBelief networks\r\nSampling from the joint distribution\r\nRecovering the joint distribution\r\nInferencing rules\r\nNa\u00efve Bayes\r\nWhy Na\u00efve Bayes is cool\r\n\r\nSupervised Learning Final Project: Using Machine Learning to Analyze Datasets\n                                    ", "end_date": null, "title": "Machine Learning 1\u2014Supervised Learning", "price": 0, "instructors": "Charles Isbell and Michael Littman", "institution_name": ["Brown University", "Georgia Institute of Technology"], "commitment": "", "subject_name": ["Computer Science"], "course_url": "https://www.udacity.com/course/ud675", "duration": null, "language_name": ["English"], "provider_name": ["Udacity"], "start_date": "2014-03-17T00:00:00Z"}